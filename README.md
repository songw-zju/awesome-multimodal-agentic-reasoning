# Awesome Multimodal Agentic Reasoning ü§ñüåêüß†
A curated collection of datasets, methods, and any other resources for Multimodal Agentic Reasoning

## Introduction
Multimodal agentic reasoning aims to enable AI systems to perceive, reason, and act within complex environments through interactive, tool-augmented, and context-aware decision-making processes. This repository provides a comprehensive list of datasets, methods, benchmarks, and resources to facilitate research and development in this emerging field.

## Datasets
| Dataset | Paper | Venue | Huggingface | GitHub | 
|:-:|:-|:-:|:-:|:-:|
||
| `GAIA` | [![arXiv](https://img.shields.io/badge/arXiv-2311.12983-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2311.12983)<br>GAIA: a benchmark for General AI Assistants| ICLR 2024 | [![Huggingface](https://img.shields.io/badge/Dataset-HuggingFace-orange?logo=huggingface)](https://huggingface.co/datasets/gaia-benchmark/GAIA) | - |
| `GTA` | [![arXiv](https://img.shields.io/badge/arXiv-2407.08713-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2407.08713)<br>GTA: A Benchmark for General Tool Agents| NeurIPS 2024 | [![Huggingface](https://img.shields.io/badge/Dataset-HuggingFace-orange?logo=huggingface)](https://huggingface.co/datasets/Jize1/GTA) | [![GitHub](https://img.shields.io/github/stars/open-compass/GTA)](https://github.com/open-compass/GTA) |
| `MLE-Bench` | [![arXiv](https://img.shields.io/badge/arXiv-2410.07095-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2410.07095)<br>MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering| arXiv 2024 | -| [![GitHub](https://img.shields.io/github/stars/openai/mle-bench)](https://github.com/openai/mle-bench) |
| `MMBench` | [![arXiv](https://img.shields.io/badge/arXiv-2307.06281-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2307.06281)<br>MMBench: Is Your Multi-modal Model an All-around Player?| ECCV 2024 | - | [![GitHub](https://img.shields.io/github/stars/open-compass/MMBench)](https://github.com/open-compass/MMBench) |
| `MLGym` | [![arXiv](https://img.shields.io/badge/arXiv-2502.14499-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2502.14499)<br>MLGym: A New Framework and Benchmark for Advancing AI Research Agents| arXiv 2025 | - | [![GitHub](https://img.shields.io/github/stars/facebookresearch/MLGym)](https://github.com/facebookresearch/MLGym) |
| `MAT` | [![arXiv](https://img.shields.io/badge/arXiv-2505.14246-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2505.14246)<br>Visual Agentic Reinforcement Fine-Tuning | arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Dataset-HuggingFace-orange?logo=huggingface)](https://huggingface.co/datasets/laolao77/MAT) | [![GitHub](https://img.shields.io/github/stars/Liuziyu77/Visual-RFT)](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT) |
| `Agent-X` | [![arXiv](https://img.shields.io/badge/arXiv-2505.24876-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2505.24876)<br>Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks| arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Dataset-HuggingFace-orange?logo=huggingface)](https://huggingface.co/datasets/Tajamul21/Agent-X) | [![GitHub](https://img.shields.io/github/stars/mbzuai-oryx/Agent-X)](https://github.com/mbzuai-oryx/Agent-X) |

## Methods
| Methods | Paper | Venue | Huggingface | GitHub | 
|:-:|:-|:-:|:-:|:-:|
||
| `Visual-ARFT` | [![arXiv](https://img.shields.io/badge/arXiv-2505.14246-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2505.14246)<br>Visual Agentic Reinforcement Fine-Tuning | arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Model-HuggingFace-orange?logo=huggingface)](https://huggingface.co/collections/laolao77/visual-arft-682c601d0e35ac6470adfe9f) | [![GitHub](https://img.shields.io/github/stars/Liuziyu77/Visual-RFT)](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT) |
| `PyVision` | [![arXiv](https://img.shields.io/badge/arXiv-2507.07998-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2507.07998)<br>Agentic Vision with Dynamic Tooling | arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Demo-HuggingFace-orange?logo=huggingface)](https://huggingface.co/spaces/Agents-X/PyVision) | [![GitHub](https://img.shields.io/github/stars/agents-x-project/PyVision)](https://github.com/agents-x-project/PyVision) |
| `DriveAgent-R1` | [![arXiv](https://img.shields.io/badge/arXiv-2507.20879-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2507.20879)<br>DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception | arXiv 2025 | - | - |

## Agentic LLMs
| Agentic LLM | Report | Venue | Huggingface | GitHub | 
|:-:|:-|:-:|:-:|:-:|
||
| `Kimi K2` | [![report](https://img.shields.io/badge/arXiv-2507.20534-b31b1b?style=flat-square&logo=report)](https://arxiv.org/abs/2507.20534)<br>Kimi K2: Open Agentic Intelligence| arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Model-HuggingFace-orange?logo=huggingface)](https://huggingface.co/collections/moonshotai/kimi-k2-6871243b990f2af5ba60617d) | [![GitHub](https://img.shields.io/github/stars/MoonshotAI/Kimi-K2)](https://github.com/MoonshotAI/Kimi-K2) |
| `GLM-4.5` | [![report](https://img.shields.io/badge/GLM-4.5-b31b1b?style=flat-square&logo=report)](https://z.ai/blog/glm-4.5)<br>GLM-4.5: Reasoning, Coding, and Agentic Abililties| 2025 | [![Huggingface](https://img.shields.io/badge/Model-HuggingFace-orange?logo=huggingface)](https://huggingface.co/zai-org/GLM-4.5) | [![GitHub](https://img.shields.io/github/stars/zai-org/GLM-4.5)](https://github.com/zai-org/GLM-4.5) |
| `gpt-oss` | [![report](https://img.shields.io/badge/gpt-oss-b31b1b?style=flat-square&logo=report)](https://openai.com/index/introducing-gpt-oss)<br>Introducing gpt-oss| 2025 | [![Huggingface](https://img.shields.io/badge/Model-HuggingFace-orange?logo=huggingface)](https://huggingface.co/openai/gpt-oss-120b) | [![GitHub](https://img.shields.io/github/stars/openai/gpt-oss)](https://github.com/openai/gpt-oss) |


## Agentic RL for LLMs
[Reinforcing Language Agents via Policy Optimization with Action Decomposition](https://arxiv.org/abs/2405.15821)

[Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning](https://arxiv.org/abs/2503.09516)

[ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)

[ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)

[Agent RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving](https://arxiv.org/abs/2505.07773)

[Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.01441)

[Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)

[Agentic Reinforced Policy Optimization](https://arxiv.org/abs/2507.19849)


## Related Surveys and Repos
[Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models)

[Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers](https://github.com/zhaochen0110/Awesome_Think_With_Images)

[Efficient Reasoning Models: A Survey](https://github.com/fscdc/Awesome-Efficient-Reasoning-Models)


## Contributing
We welcome contributions from the community! Feel free to submit pull requests or issues to add:
- New datasets and benchmarks.
- Novel methods and models.
- Toolkits, frameworks, or evaluation protocols.
- Relevant papers and resources.

