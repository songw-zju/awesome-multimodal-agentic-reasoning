# Awesome Multimodal Agentic Reasoning ü§ñüåêüß†
A curated collection of datasets, methods, and any other resources for Multimodal Agentic Reasoning

## Introduction
Multimodal agentic reasoning aims to enable AI systems to perceive, reason, and act within complex environments through interactive, tool-augmented, and context-aware decision-making processes. This repository provides a comprehensive list of datasets, methods, benchmarks, and resources to facilitate research and development in this emerging field.

## Datasets
| Dataset | Paper | Venue | Huggingface | GitHub | 
|:-:|:-|:-:|:-:|:-:|
||
| `MAT` | [![arXiv](https://img.shields.io/badge/arXiv-2505.14246-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2505.14246)<br>Visual Agentic Reinforcement Fine-Tuning | arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Dataset-HuggingFace-orange?logo=huggingface)](https://huggingface.co/datasets/laolao77/MAT) | [![GitHub](https://img.shields.io/github/stars/Liuziyu77/Visual-RFT)](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT) |
| `Agent-X` | [![arXiv](https://img.shields.io/badge/arXiv-2505.24876-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2505.24876)<br>Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks| arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Dataset-HuggingFace-orange?logo=huggingface)](https://huggingface.co/datasets/Tajamul21/Agent-X) | [![GitHub](https://img.shields.io/github/stars/mbzuai-oryx/Agent-X)](https://github.com/mbzuai-oryx/Agent-X) |

## Methods
| Methods | Paper | Venue | Huggingface | GitHub | 
|:-:|:-|:-:|:-:|:-:|
||
| `Visual-ARFT` | [![arXiv](https://img.shields.io/badge/arXiv-2505.14246-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2505.14246)<br>Visual Agentic Reinforcement Fine-Tuning | arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Model-HuggingFace-orange?logo=huggingface)](https://huggingface.co/collections/laolao77/visual-arft-682c601d0e35ac6470adfe9f) | [![GitHub](https://img.shields.io/github/stars/Liuziyu77/Visual-RFT)](https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT) |
| `PyVision` | [![arXiv](https://img.shields.io/badge/arXiv-2507.07998-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2507.07998)<br>Agentic Vision with Dynamic Tooling | arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Demo-HuggingFace-orange?logo=huggingface)](https://huggingface.co/spaces/Agents-X/PyVision) | [![GitHub](https://img.shields.io/github/stars/agents-x-project/PyVision)](https://github.com/agents-x-project/PyVision) |
| `DriveAgent-R1` | [![arXiv](https://img.shields.io/badge/arXiv-2507.20879-b31b1b?style=flat-square&logo=arxiv)](https://arxiv.org/abs/2507.20879)<br>DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception | arXiv 2025 | - | - |

## Agentic LLMs
| Agentic LLM | Report | Venue | Huggingface | GitHub | 
|:-:|:-|:-:|:-:|:-:|
||
| `Kimi K2` | [![report](https://img.shields.io/badge/arXiv-2507.20534-b31b1b?style=flat-square&logo=report)](https://arxiv.org/abs/2507.20534)<br>Kimi K2: Open Agentic Intelligence| arXiv 2025 | [![Huggingface](https://img.shields.io/badge/Model-HuggingFace-orange?logo=huggingface)](https://huggingface.co/collections/moonshotai/kimi-k2-6871243b990f2af5ba60617d) | [![GitHub](https://img.shields.io/github/stars/MoonshotAI/Kimi-K2)](https://github.com/MoonshotAI/Kimi-K2) |


## Related Surveys and Repos
[Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models](https://github.com/HITsz-TMG/Awesome-Large-Multimodal-Reasoning-Models)

## Contributing
We welcome contributions from the community! Feel free to submit pull requests or issues to add:
- New datasets and benchmarks.
- Novel methods and models.
- Toolkits, frameworks, or evaluation protocols.
- Relevant papers and resources.

